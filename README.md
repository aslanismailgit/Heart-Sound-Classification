Heart Sound Audio Classification (HSC) Using Deep Neural Networks  
HSC problem has been taking researchers’ attention since the Cardio Vascular Disease (CVD) has a major ratio in the death rates around the world. One of the problems we are of interest in the field is the classification of Heart Sounds as Normal or Abnormal. The problem turns out to be well know supervised classification problem in the Machine Learning area. 
Researchers applied different ML techniques including supervised, semi-supervised and unsupervised types of approaches and applied LSTM, RNN, CNN in many different studies. 
Many studies focus on “feature extraction” phase by using different techniques. For example, in [“Murmur Detection Using Parallel Recurrent & Convolutional Neural Networks” Shahnawaz Alam, Rohan Banerjee, Soma Bandyopadhyay (2018)] authors used “Cepstrogram (MFCC) for the LSTM model and Spectrogram for the CNN model as two sets of features”. The examples of using those kind of features are not limited to mentioned paper.
My aim was to develop a model which uses signal samples as an input to the DNN model. I used CinC dataset. I have divided each signal into 5 sec pieces. (and 0.1 sec time windows, having a 50x200 samples) In fact, while applying LSTM and CONV1D, I used 5 sec pieces as a whole, making a 10.000 x 1 vector. I divided the dataset into train, validation, and test set each having 9761, 1952, 1302 (%75, %15, %10) record pieces of 5 secs. I created 10 randomly selected datasets each having train, validation, and test sets.
Even though different hyper parameters may result in different results, some of my deductions are- some of them already well-known facts by the experienced people, hope somebody comment on my writings:
-	LSTM, which is thought to be good for sequential data, did not give good results on HSC problem. (see above LSTM model). It requires too much time and memory. No better results than CONV1D. (See results.xlsx)
-	CONV1D shows the best performance. As stated in [Deep Learning With Python, François Chollet (2018)], “Such 1D convnets can be competitive with RNNs on certain sequence-processing problems, usually at a considerably cheaper computational cost. Recently, 1D convnets, typically used with dilated kernels, have been used with great success for audio generation and machine translation. In addition to these specific successes, it has long been known that small 1D convnets can offer a fast alternative to RNNs for simple tasks such as text classification and timeseries forecasting.”
-	Deeper models make the results worse. The deeper does not always mean the better. You should “fit” your model architecture to your problem and dataset. (see model_audio_custom_resnet.py)
-	Since I divided the same audio record into pieces, in some cases, one piece of the same record is classified as positive and another piece as negative. That makes sense, since some of them has noise (for example in one record you hear a nearby mobile phone’s bip,bip sound.)
-	As we all know, metrics that are of interest depends on what our expectation from the model. Most studies used sensitivity and specificity as evaluation metrics. One thing we need to be aware of is, In tf. keras.metrics.SensitivityAtSpecificity(0.5), as seen here, the best sensitivity is at a given specifity, for example 0.5. However, the confuision matrix gives different values at (y_prob>0.5). (Computes best sensitivity where specificity is >= specified value. https://keras.io/api/metrics/classification_metrics/). Of course, the second threshold (0.5) is different than the previous one. The first one indicates the best sensitivity while specifity = 0.5 which can be %100, leading a misevaluation. 

